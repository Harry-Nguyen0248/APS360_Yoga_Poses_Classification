{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import the data and Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "import shutil\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "random.seed(30)\n",
    "\n",
    "#Path to dataset \n",
    "#TBD\n",
    "data_dir = 'TBD'\n",
    "\n",
    "#Get all individuals \n",
    "individuals = [name for name in os.listdir(data_dir) \n",
    "               if os.path.isdir(os.path.join(data_dir, name))]\n",
    "\n",
    "#Split individuals into train, val, test sets \n",
    "#(30% random for combined validation and test set)\n",
    "train_individuals, test_individuals = train_test_split(individuals, \n",
    "                                                       test_size=0.30, \n",
    "                                                       random_state=30)\n",
    "#(50% random for validation and 50 for test set)\n",
    "val_individuals, test_individuals = train_test_split(individuals, \n",
    "                                                     test_size=0.50, \n",
    "                                                     random_state=30)\n",
    "\n",
    "\n",
    "#create directories for the splits (TBD)\n",
    "train_dir = 'TBD'\n",
    "val_dir = 'TBD'\n",
    "test_dir = 'TBD'\n",
    "\n",
    "os.makedirs(train_dir, exist_ok=True)\n",
    "os.makedirs(val_dir, exist_ok=True)\n",
    "os.makedirs(test_dir, exist_ok=True)\n",
    "\n",
    "#Define a function to copy images to new directories\n",
    "def copy_images(individuals, source_dir, dest_dir):\n",
    "    for individual in individuals:\n",
    "        individual_dir = os.path.join(source_dir, individual)\n",
    "        if os.path.isdir(individual_dir):\n",
    "            shutil.copytree(individual_dir, os.path.join(dest_dir, \n",
    "                                                         individual), \n",
    "                                                         dirs_exist_ok=True)\n",
    "\n",
    "copy_images(train_individuals, data_dir, train_dir)\n",
    "copy_images(val_individuals, data_dir, val_dir)\n",
    "copy_images(test_individuals, data_dir, test_dir)\n",
    "\n",
    "#Load dataset via ImageFolder\n",
    "train_dataset = ImageFolder(train_dir)\n",
    "val_dataset = ImageFolder(val_dir)\n",
    "test_dataset = ImageFolder(test_dir)\n",
    "\n",
    "#Check output of each set\n",
    "print(f\"Number of training images: \", len(train_dataset))\n",
    "print(f\"Number of validation images: \", len(val_dataset))\n",
    "print(f\"Number of test images: \", len(test_dataset)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Build CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class YogaClassfier(nn.Module):\n",
    "    def __init__(self, num_classes = 9):\n",
    "        super(YogaClassfier, self).__init__()\n",
    "\n",
    "        #Convolutional layers (increasing numbers of filters)\n",
    "        self.conv1 = nn.Conv2d(in_channels = 3, out_channels=64, \n",
    "                               kernel_size = 3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels = 64, out_channels=128, \n",
    "                               kernel_size = 3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv2d(in_channels = 128, out_channels=256, \n",
    "                               kernel_size = 3, stride=1, padding=1)\n",
    "        self.conv4 = nn.Conv2d(in_channels = 256, out_channels=512, \n",
    "                               kernel_size = 3, stride=1, padding=1)\n",
    "        \n",
    "        #Max-pooling layers \n",
    "        #reducing spatial dimensinons by half after each layer\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "\n",
    "        #Fully connected layers\n",
    "        self.fc1 = nn.Linear(in_features=512 * 14 * 14, out_features=1024)\n",
    "        self.fc2 = nn.Linear(in_features=1024, out_features=512)\n",
    "        self.fc3 = nn.Linear(in_features=512, out_features=num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #Convlutional layers with ReLU and pooling\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "        x = self.pool(F.relu(self.conv4(x)))\n",
    "\n",
    "        #Flatten the feature map\n",
    "        x = x.view(-1, 512*14*14)\n",
    "\n",
    "        #Fully connected layers with ReLU\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x) #handled by loss fuction \n",
    "\n",
    "        return x\n",
    "\n",
    "model = YogaClassfier(num_classes=9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Train CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Sanity check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Tranfer Learning"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
